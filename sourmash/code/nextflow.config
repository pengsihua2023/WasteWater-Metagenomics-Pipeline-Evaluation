/*
 * Nextflow configuration
 * For the Sourmash metagenomic analysis workflow
 */

// Workflow configuration
// Note: the default workDir may be overridden by the --work_dir parameter
// The output directory is controlled by the --outdir parameter
workDir = 'work'

// Default parameters (command-line `--xxx` overrides these)
params {
    // Slurm
    slurm_partition = 'bahl_p'

    // Default walltime (can be overridden per-process via withName/withLabel)
    default_task_time = '144h'
    low_task_time     = '24h'
    medium_task_time  = '48h'
    high_task_time    = '144h'

    // Assembly resources (can be overridden by run_combined.sh parameters)
    megahit_task_memory  = '240 GB'
    megahit_task_cpus    = 16
    megahit_task_time    = '144h'
    metaflye_task_memory = '128 GB'
    metaflye_task_cpus   = 16
    metaflye_task_time   = '144h'
}

// Process configuration
process {
    // Slurm partition (sbatch --partition); can be overridden via --slurm_partition
    queue = params.slurm_partition
    
    // Important: some Slurm clusters require explicit job size (at least --ntasks),
    // otherwise you may see: "You must specify a number of tasks."
    // Force each Nextflow task to submit as 1 task on 1 node.
    clusterOptions = '--nodes=1 --ntasks=1'
    
    // Important: some Slurm clusters require explicit --time.
    // If a process has no time, you may see: "Time limit specification required, but not provided"
    // Set a default walltime for all tasks; override per process as needed.
    // Default is 144h to cover large assemblies.
    time = params.default_task_time

    // Default resource profiles
    withLabel: 'low_memory' {
        memory = '32 GB'
        cpus = 8
        time = params.low_task_time
    }
    
    withLabel: 'medium_memory' {
        memory = '64 GB'
        cpus = 8
        time = params.medium_task_time
    }
    
    withLabel: 'high_memory' {
        // Note: on Slurm, this becomes the job cgroup memory limit.
        // If too small, memory-hungry steps (e.g. MEGAHIT) may be OOM-killed (SIGKILL/-9).
        memory = '128 GB'
        cpus = 16
        time = params.high_task_time
    }
    
    // Per-process overrides
    withName: 'META_SPADES' {
        memory = '256 GB'
        cpus = 16
        time = '72h'
    }
    
    withName: 'MEGAHIT_ASSEMBLY' {
        // Notes:
        // - MEGAHIT can be extremely memory-hungry on large datasets (e.g. billions of reads).
        // - Under Slurm, memory here must be sufficiently large or cgroups may kill the process.
        // - You can override via --megahit_task_memory/--megahit_task_cpus at runtime.
        memory = params.megahit_task_memory
        cpus   = params.megahit_task_cpus
        time   = params.megahit_task_time
    }
    
    withName: 'METAFLYE_ASSEMBLY' {
        // Note: some Slurm clusters enforce --time; set it explicitly to avoid submission failures.
        memory = params.metaflye_task_memory
        cpus   = params.metaflye_task_cpus
        time   = params.metaflye_task_time
    }
    
    withName: 'SKETCH_CONTIGS' {
        memory = '32 GB'
        cpus = 8
        time = '24h'
    }
    
    withName: 'SKETCH_READS' {
        memory = '32 GB'
        cpus = 8
        time = '24h'
    }
    
    withName: 'VIRAL_GATHER' {
        memory = '64 GB'
        cpus = 8
        time = '24h'
    }
    
    withName: 'BACTERIAL_GATHER' {
        memory = '64 GB'
        cpus = 8
        time = '24h'
    }

    withName: 'GENERATE_REPORT' {
        // Note: some Slurm clusters require an explicit memory request (--mem/--mem-per-cpu).
        // Without memory, sbatch can fail with: "You must request some amount of memory."
        // Report generation is lightweight; small resources are sufficient.
        memory = '4 GB'
        cpus = 1
        time = '2h'
    }
    
    // Error retries
    errorStrategy = 'retry'
    maxRetries = 2
    
    // Cleanup work directory
    cleanup = true
}

// Executor configuration
executor {
    // Slurm execution: Nextflow submits sbatch per process
    name = 'slurm'
    queueSize = 50
}

// Apptainer/Singularity configuration
// Note: enable only one container engine (do not enable both singularity and apptainer)

// Use Apptainer (recommended; successor to Singularity)
apptainer {
    enabled = true
    autoMounts = true
    cacheDir = '/scratch/sp96859/Meta-genome-data-analysis/Apptainer/cache'
    // Or via environment variables
    // cacheDir = System.getenv('APPTAINER_CACHEDIR') ?: System.getenv('SINGULARITY_CACHEDIR') ?: './containers'
}

// If the system provides Singularity but not Apptainer, disable apptainer above and enable singularity below
/*
singularity {
    enabled = true
    autoMounts = true
    cacheDir = '/scratch/sp96859/Meta-genome-data-analysis/Apptainer/cache'
    // Or via environment variables
    // cacheDir = System.getenv('SINGULARITY_CACHEDIR') ?: './containers'
}
*/

// Container configuration (Docker; uncomment if needed)
// docker {
//     enabled = false
// }

// Report configuration
report {
    enabled = true
    file = "${params.outdir}/nextflow_report.html"
    // Allow overwriting existing report file (optional but recommended)
    overwrite = true
}

// Timeline configuration
timeline {
    enabled = true
    file = "${params.outdir}/nextflow_timeline.html"
    // Allow overwriting existing timeline file (optional but recommended)
    overwrite = true
}

// Trace configuration
trace {
    enabled = true
    file = "${params.outdir}/nextflow_trace.txt"
    // Allow overwriting existing trace file (avoid failure on reruns)
    overwrite = true
    fields = 'task_id,hash,native_id,process,tag,name,status,exit,module,container,cpus,time,disk,memory,attempt,submit,start,complete,duration,realtime,queue,%cpu,%mem,rss,vmem,peak_rss,peak_vmem,rchar,wchar,syscr,syscw,read_bytes,write_bytes,vol_ctxt,inv_ctxt'
}

